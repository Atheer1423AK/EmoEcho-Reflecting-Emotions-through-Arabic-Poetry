# ğŸ­ EmoEcho: Reflecting Emotions through Arabic Poetry

## ğŸ“œ Project Overview
Arabic poetry carries deep cultural and emotional significance, yet its automated analysis remains a challenging task due to its linguistic complexity and metaphorical expressions. While much of Arabic NLP has focused on prose and dialects, poetryâ€”especially emotional classification in verseâ€”has been underexplored.

**EmoEcho** tackles this gap by classifying emotions in Arabic poetry using a curated dataset labeled for **ğŸ˜Š joy**, **ğŸ˜¢ sadness**, and **â¤ï¸ love**.  
This work supports:
- ğŸ› Cultural preservation and emotion-aware digital libraries
- ğŸ“ Educational tools for literary studies in digital humanities

Two deep learning approaches were evaluated:
1. ğŸ¤– **AraBERT** â€” fine-tuned specifically for poetry  
2. ğŸŒ€ **CNN-LSTM hybrid** â€” designed for sequential language processing  

**Top Results**:
- **AraBERT**: ğŸ¥‡ **75.1% Accuracy** | **0.7512 F1-score**  
- **CNN-LSTM**: ğŸ“‰ Lower performance, highlighting transformer modelsâ€™ advantage in capturing poetic nuances

---

## ğŸ¯ Objectives
- Bring **emotion classification** to Arabic poetry
- Use advanced **NLP models tailored for Arabic text** (AraBERT)  
- Explore hybrid approaches combining **CNN with LSTM**
- Enable practical cultural applications â€” from digital archives to teaching tools

---

## ğŸ“‚ Dataset
ğŸ“Œ **Source**: [Arabic Poetry Emotions Dataset on Kaggle](https://www.kaggle.com/datasets/sakibshahriar95/arabic-poetry-emotions-dataset)  

A dataset of Arabic poems categorized into three emotions:
- ğŸ˜Š Joy  
- ğŸ˜¢ Sadness (including lamentation)  
- â¤ï¸ Love  

**Preprocessing steps**:
- ğŸ”¤ Normalization  
- âœ‚ï¸ Tokenization  
- ğŸ“ Diacritic removal  
- âš– Handling class imbalance through up-/down-sampling

---

## ğŸ›  Methodology
1. ğŸ§¹ **Preprocess Text** â€“ Normalize, tokenize, remove diacritics if needed  
2. ğŸ“š **Build Embeddings** â€“ Use AraBERT tokenizer or Word2Vec (for CNN-LSTM)  
3. ğŸ‹ï¸ **Train Models**:
   - Fine-tune **AraBERT**
   - Train **CNN-LSTM hybrid**
4. ğŸ“Š **Evaluate** â€“ Accuracy, Precision, Recall, F1-score, and Confusion Matrix  

---

## ğŸ“ˆ Results

| ğŸ§  Model         | ğŸ¯ Accuracy | ğŸ“Š F1-score |
|------------------|------------:|------------:|
| ğŸ¤– AraBERT       | 75.1%       | 0.7512      |
| ğŸŒ€ CNN-LSTM      | ~59%        | ~0.64       |

âœ… AraBERT significantly outperformed CNN-LSTM, underscoring its strength in handling the nuanced language of Arabic poetry.

